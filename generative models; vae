reading from "https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73"
and "https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29"

''''

之前我所了解的大多是 discriminative model，对于一个分类任务，这种模型它主要的任务就是在学习一个函数f(x,y) = p(y|x)，也就是
给定一个输入，我们想要知道它在不同类别下的概率值是多少；

generative model则会对x和y的联合分布p(x,y)建模


variational autoencoder:

in just three years, variational autoencoers have emerged as one of the most popular approaches
to unsupervised learning of complicated distributions.  vaes are appealing because they are built
on top of standard function approximators (neural networks), and can be trained with stochastic
gradient descent. vaes have already shown promise in generating many kinds of complicated data,
including handwritten digits, faces, house numbers, cifar images, physical models of scenes, 
segmentation, and predicting the future from static images. this tutorial introduces the intuitions
behind vaes, explains the mathematics behind them, and describes some empirical behavior.

introduction;

generative modeling is a broad area of machine learning which deals with models of distribution

one most straightforward and simplest generative model: model the images in our training dataset;
we could estimate a p(x), when we have a new image, if this image is similar to the training images,
p(x) would be high, otherwise, p(x) would be low if this image is a random noise.
but this kind of generative model is not useful, because it is not useful if we want to synthesize
a new image like the training images.


instead, one often cares about producing more examples that are like those already in a database,
我们在乎的是生成更多的样本，这些样本很像数据集中的样本，但又不完全一样, but not exactly the same.
we could start with a database of raw images

给模型一个输入，这个encoder此时负责的是根据这个输入得到mean和covariance for 一个高维高斯分布，这个高维高斯分布
的具体维度就是latent space的维度，p(z|x)的意思是，given x，z的概率分布/密度函数。
接着我从这个高斯分布中随机抽取z，用decoder还原z；
再结合上我们的loss function分析，这个时候，KL散度项是限制每种类型图片的latent features的分布，假设我们是在mnist
数据集上做，那么我们就是希望0-9这10种图像的latent features的分布都能够逼近标准正态分布（实际当然不可能），这样的话
它们就能很紧凑（具体目前没想到有什么好处），但是它们的latent features的分布又不会成为标准正态分布，因为它们不允许重合，
因为我们还有reconstruction term,试想如果都重合了，那就无法完成重构任务，那么损失就会很大；所以为了减小损失，损失函数
会迫使10种图片的latent features分布们不会完全重合到标准正态分布；同时重构损失会

intuitions about the regularization
the regularity that is expected from the latent space in order to make generative process possible can be expressed 

vae并不是想要通过encoder推理出输入在latent space中的latent feature，而是想要推测出输入的latent feature的概率分布；
decoder也是拥有很强的general ability，他要学的不是将一个latent feature映射回original input，而是map the latent features 
within the distribution to original input

continuity (two close points in the latent space should not give two completely different contents once decoded) and
completeness (for a chosen distribution, a point sampled from the latent space should give "meaningful" content once
decoded)

the only fact that vaes encode inputs as distributions instead of simple points is not sufficient to ensure
continuity and completeness. without a well defined regularization term, the model can learn, in order to minimize
its creconstruction error, to "ignore" the fact that distributions are returned and behave almost like classic 
autoencoders (leading to overfitting). to do so, the encoder can either return distributions with tiny variances (that 
would tend to be punctual distributions) or return distributions with very different means (that would then be really
far apart from each other in the latent space). in both cases, distributions are used the wrong way (cancelling 
the expected benefit) and continuity or completeness are not satisfied.

如果不施加额外的约束，只是简单的寄希望于将input map to distribution来实现我们的目的，那最终还是不行；因为encoder可以将每个输入样本的
distribution映射成（以gaussian为例）方差极小的，并且各distribution的平均值相差很远的分布们；这样的其实还是很严重的overfitting；
我们随意的在latent space中取值，但凡不是取到那些如同针一样的gaussian上，decoder就无法解码出有意义的图，甚至离“针”很近的情况下
decoder就是不能解码出跟那个”针‘相似的输入。

so, in order to avoid these effects we have to regularize both the covariance matrix and the mean of the distributions
returned by the encoder; (VAE ENCODER RETURN mean vector AND covariance matrix FOR THE DISTRIBUTION OF LATENT
FEATURES OF INPUT). in practice, this regularization is done by enforcing distributions to be close to a standard
normal distribution (centered and reduced). this way, we require the covariance matrices to be close to the identity, 
preventing punctual distributions, and the mean to be close to 0, preventing encoded distributions to be too far 
apart from each others.

其中的一个regularization term是要将所有的inputs的latent features的distributions们都约束到一个standard normal distribution上，
这样以来，大家的means都会向0靠拢，不至于说相差甚远导致各distribution之间出现“空白区域”（decoder理解不了的latent features），另一
方面，将大家的variance都会向1靠拢，如此一来不会出现说，那种像“针”一样的latent features分布，也就是latent feature distribution
泛化能力好，这一片都可以被decode to相似的输入；

with this regularization term, we prevent the model to encode data far apart in the latent space and encourage 
as much as possible returned distributions to "overlap", satisfying this way the expected continuity and completeness
conditions. naturally, as for any regularization term, this comes at the price of a higher reconstruction error
on the training data. the tradeoff between the reconstruction error and the KL convergence can however be adjusted
and we will see the next section how the expression of the balance naturally emerge from our formal derivation.

KL convergence regularization term去约束各latent feature distributions去逼近standard normal distribution会导致重构误差变大，
所以需要一个tradeoff；

to conclude this subsection, we can observe that continuity and completeness obtained with regularization tend to
create a "gradient" over the information encoded in the latent space. for example, a point of the latent space that
would be halfway between the means of two encoded distributions coming from different training data should be decoded
in something that is somewhere between the data 

总之，distributions们的variance过于小不是一件好事，过于小首先会导致问题1:各Latent feature distributions如同针一样，distribution
稍微靠外围一点的latent feature就无法被decode to original input；同时如果variance相对于各个分布的means之间的距离来说过于小的话，那么
各分布就会相隔很远，导致分布之间的点无法有意义地被decode

mathematical details of VAEs
in the previous section we gave the following intuitive overview: vaes are autoencoders that encode inputs as
distributions instead of points and whose latent space "organization" is regularized by constraining distributions
returned by the encoder to be close to a standard Gaussian. in this section, we will give a more mathematical view of
VAEs that will allow us to justify the regularization term more rigorously. to do so, we will set a clear 
probabilistic framework and will use, in particular, variational inference technique.




"""  一些新的关于VAE的体会"""

首先是VAE的encoder is not deterministic, it is not used to map input image to a single point 
in latent space; but find the necessary parameters for probability distribution p(z|x);
here we need an assumption for p(z|x), if we assume it is a Gaussian distribution, then, encoder
is responsible for finding the mean and covariance;

the second assumption is, we assume all p(z|x) should be close to a prior distribution for p(z);
for example, here we will assume p(z) is a standard normal distribution N(0, I);

Then another assumption is, based on initialized weights for decoder, the decoder could be a 
function for z - f(z); we assume p(x|z) = N(f(z), cI) where f(z) and cI are initialized;

so now, we know p(z), p(x|z), based on bayesian theory, we could compute p(z|x) = p(x|z)p(z)/p(x);
where p(x) = sum_z[p(x|z)p(z)] and p(x) should be a constant because we have fixed p(z) and 
p(x|z) by assumption and initialization; however, it will be computationally complicated to 
compute p(x), just say if we have a z in 100-dimensional, it will be a summation for 2^100 for
a single p(x_i). therefore, if we want to get p(z|x), we need some methods to avoid computing
p(x);

therefore, people choose to use variational inference method to approximate p(z|x);
1. 我们需要假设一个概率分布函数的family,这个family的概率分布基础函数是假设好的,比如我们假设它是一个高斯函数,
那么这个family是由无数个高斯概率分布函数组成的，他们的mean和variance要根据encoder的weights来决定；
我们的目的就是从这个family中找到最好的那个so that 它和我们的目标概率分布函数p(z|x) = Cp(x|z)p(z)最相似；

这里有的人会使用mean field family, 它的意思就是，这个family里的每个成员函数都符合以下特性：
f(z) = multiply_j[f_j(z_j)], 就是说对于我们的高维度latent feature vector, lets say 100-d，
这个100-d vector的概率分布函数可以拆分为 每一维单独有一个自己的1-d gaussian, 自己的scalar mean and 
variance value, 然后100个1-d gaussian的积，是最终的f(z);
我感觉这里这样做，是为了方便使用神经网络，这样的话如果我们想要输入图片的latent features是在一个100-d latent
space，我们可以设置encoder的输出是200 nodes, 100 scalar means and 100 scalar variances for 100
1-d gaussian of each element within 100-d latent feature vector；


""" mathematic stuffs behind vae;
let's begin by defining a probabilistic graphical model to describe our data,
we denote by x the variable that represents our data and assume that x is generated from
a latent variable z (the encoded representation) that is not directly observed. thus for each
data point, the following two steps generative process is assumed:

first a latent representation z is sampled from the prior distribution p(z);
second the data x is sampled from the conditional likelihood distribution p(x|z);

contrary to a simple autoencoder that consider deterministic encoder and decoder, we are 
going to consider now probabilistic versions of these two objects. 

here we are going to approximate p(z|x) by a gaussian distribution q_x(z) whose mean and 
convariance are defined by two functions, g and h, of the parameter x.

we assume the function f in decoder known and fixed, under this assumptions, we could 
approximate the posterior p(z|x) using variational inference technique. however,
in practice, this function f that defines the decoder is not known and also need to be 
chosen. to do so, let's remind that our initial goal is to find a performant encoding-decoding
scheme whose latent space is regular enough to be used for generative purpose. if the regularity
is mostly ruled by the prior distribution assumed over the latent space, the performance of
the overall encoding-decoding scheme highly depends on the choice of the function f.

main takeaways:
1. dimensionality reduction is the process of reducing the number of features that describe
some data (either by selecting only a subset of the initial features or by combining them
into a reduced number new features) and so, can be seen as an encoding process.

2. autoencoders are neural networks architectures composed of both an encoder and a decoder 
that create a bottleneck to go through for data and that are trained to lose a minimal quantity
of information during the encoding-decoding process (training by gradient descent iterations
with the goal to reduce the reconstruction error)

3. due to overfitting, the latent space of an autoencoder can be extremely irregular (close 
points in latent space can give very different decoded data, some point of the latent space can
give meaningless content once decoded, ...) and, so, we cant really define a generative process
that simply consists of sampling a point from the latent space and make it go through the 
decoder to get a new data.

4. variational autoencoder (VAEs) are autoencoders that tackle the problem of the latent 
space irregularity by making the encoder return a distribution over the latent space instead 
of a single point and by adding in the loss function a regularization term over that returned 
distribution in order to ensure a better organization of the latent space

5.


















