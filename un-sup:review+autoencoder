review the probability theory:

x: 成功夺冠
y: 训练期天天睡觉；训练期天天训练

prior probability of x: p(x)
conditional probability of x: p(x|y = 训练期天天训练)
posterior probability of x: p(y|x)


review of the information and entropy:
if X represents the even for which country will win 2026 world cup; and this special world cup will only have China, US, and
France

p(X=China): 1/100;
p(X=US):29/100;
p(X=France):70/100;

如果别人给你一个信息：法国队夺冠了，你不会觉得惊讶，因为法国队夺冠的prior概率很高，所以这一个信息不够爆炸，你会觉得信息量很低
但是如果告诉你：中国队夺冠了，你会觉得非常震惊，因为中国队夺冠的prior概率很低，所以这个信息很劲爆，你会觉得信息量十足

那么这个信息量的量化公式就是 -log2(p(X));这件事情的先验概率越高，越接近于1，那么它最终的信息量就会变成0；反而如果这件事的先验概率很低，
那么最终它发生后的信息量就会很高；

如果说，信息量是用来衡量这个事件最终结果的劲爆程度，那么我们还有一个定义，信息熵

对于这么一个，可能会有多种结果的事件X，在它具体结果出现之前，我们可以根据它各种情况的先验概率得到这个事件最终信息量的期望值，这个期望值
就是信息熵；如果这个事件的各种结果的概率分布非常单一，类似于0.97，0.02，0.01，那么信息量期望值就会低，信息熵也会低；信息熵高会说明信息量的期望值高，也就是说这个事件
没有一个结果它的发生先验概率明显高于其他结果，可能各结果发生的先验概率分布接近，类似于 0.28，0.32，0.40

entropy: sum( p(x)*-log2(p(x)) )


pca

suppose you are training your algorithm on images. then the input will be somewhat redundant, because the values
of adjacent pixels in an image are highly correlated. concretely, suppose we are training on 16x16 grayscale image
patches. then x <- r^256 are 256 dimensional vectors, with one feature xj corresponding to the

pca是对样本们的特征维度进行操作的：假设我们现在有300张图片，每张图是10x10像素点，也就是说，现在我们有300个样本，每个样本由100个特征值来表示；
那么这个时候我们样本 X:300x100；每个样本xi会有100个特征；这里我们对每一个特征维度做一次标准化，也就是做100次，让每个特征维度内部的均值为0

对于一号特征f1，我们现在有300个这样的值，然后如果我们单看这一个特征，就好比是这300个样本散落在一个100维的空间里，然后每个样本在向量基base_1
上的投影值，就是f1的值们，但是之所以我们想要降维度：是因为维度太高，数据量太大，这里面存在很多冗余的特征纬度；就假设特征1 f_1，假设这300
个样本的f_1们都是一样的，那么这个特征1号对于区分这300个样本根本毫无贡献，所以我们想要去除掉这些特征内部方差为0的特征们；

另一方面除去特征内部的方差，我们还要考虑一下特征之间的方差，也就是协方差，假设特征2号和特征3号 f_2和f_3的协方差很高，那么就说明这两个特征维度
的相关性很高，就好比如果f_2值为1到100， 而f_3值为10-1000， 那么他们其实是线性相关的，对于区分这三百个样本来说，只需要这两个特征中的一个就行了

因此我们既需要考虑特征内部方差，也需要考虑特征间的协方差

X = 100 x 300; X^T = 300 x 100; 它们的矩阵相称 XX^T = 100 x 100; 这个矩阵的对角线，是这一百个特征的内部方差，对角线两边的都是协方差；
我们希望对X做一些变化 使得新的;



sparse autoencoder;
相比于undercomplete autoencoder, sparse ae拥有更复杂的网络结构; 如果说undercomplete ae是想要通过少量的hidden 
nodes去找到合适的latent features to abstractly represent input data; 那么sparse ae则是希望通过loss
function来限制每个样本可以使用的hidden node数量，也就是让一些node不被activated;

为此设计了两种不同的loss function;

第一种是加一个L1 term, 只不过这一次的term不是去约束weights的大小,而是 lambda*sum_i(|a(x_i)|_1)


第二个是:
rou_hat_j = avg( sum_i( a_j( x_i ) ) ); 它包含了一个batch里所有input在j_th node处的激活的平均值;
我们是想要把这个值近似成一个j_th node 激活/不激活的一个二项分布, rou_hat_j 就是激活的概率;

同时我们给所有hidden nodes设定好一个rou; 这个是理想的,目标的hidden nodes们激活/不激活的二项分布

我们希望用j个KL conv去衡量每个node的分布是否足够接近我们的理想分布rou, 如果接近, 那么KL conv自然就会很小, 
loss function也会变小;


This loss term is visualized below for an ideal distribution of rou = 0.2, corresponding with
the minimum (zero) penalty at this point



Denoising autoencoder:
so far we've discussed the concept of training a neural network where the input and 
outputs are identical and our model is tasked with reproducing the input as closely as
possible while passing through some sort of information bottleneck. recall thta i mentioned
we'd like our autoencoder to be sensitive enough 

another approach towards developing a generalizable model is to slightly corrupt the input 
data but still maintain the uncorrupted data as our target output.













