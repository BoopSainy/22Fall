TransGeo (checked); Polar + SAFA; VIGOR; L2LTR; Generative;

Title: Local Attention for Geo-Localization with High Resolution Cross-View Images

Polar Transformer; SAFA;

-1-: TransGeo: (ASAM(details), polar transform, SAFA)

abstract:

过去的主流是使用基于CNN的模型去解决, TransGeo是首个只使用pure-transformer的框架，利用了ViT的全局建模能力和位置编码，
接着通过attention map将注意力权重低的部分移除，提高注意力高的部分的像素
The dominant CNN-based methods for cross-view image geo-localization rely on polar transform and fail to model the global 
correlation. We propose a pure transformer-based approach (TransGeo) to address these limitations from a different
perspective. TransGeo takes full advantage of the strengths of transformer related to global information modeling and
explicit position information encoding. we further leverage the flexibility of transformer input and propose an attention-
guided non-uniform cropping method, so that un-informative image patches are removed with negligible drop on 
performance to reduce computation cost. The saved computation can be reallocated to increase resolution only for 
informative patches, resulting in performance improvement with no additional computational cost. This "attend and zoom-in"
strategy is highly similar to human behavior when observing images. Remarkably, TransGeo achieves state-of-the-art results
on both urban and rural datasets, with siginificantly less computation cost than CNN-based methods. It does not rely on
polar transform and infers faster than CNN-methods.

introduction:

Image-based geo-localization aims to determine the location of a query street-view image by retrieving the most similar
images in a GPS-tagged reference database. It has great potential for noisy GPS correction and navigation in crowed cities.
Due to the complete coverage and easy access of aerial images from Google-Map API, a thread of works focus on cross-view
a thread of works focus on cross-view geo-localization, where the satellite/aerial images are collected as reference
images for both rural and urban areas. They generally train a two-stream CNN (convolutional Neural Network) framework
employing metric learning loss. However, such cross-view retrieval systems suffer from the great domain gap between 
street and aerial view, as CNNs do not explicitly encode the position information of each view.

To bridge the domain gap, recent works apply a predefined polar transform on the aerial-view images. The transformed aerial
images have a similar geometry layout as the street-view query images, which results in siginificant boost in the retrieval
performance. However, the polar transformer relies on the prior knowledge of the geometry corresponding to the two views,
and may fail when the street query is not spatially aligned at the center of aerial images.

Recently, vision transformer[] has achieved significant performance on various vision tasks due to its powerful global
modeling ability and self-attention mechanism. Although CNN-based are still predominant for cross-view geo-localization,
we argue vision transformer is more suitable for this task due to three advantages: 1) Vision Transformer explicitly 
encodes the position information, thus can directly learn the geometric correspondence between two views with the 
learnable position embedding. 2) The multi-head attention [28] module can model global long-range correlation between
all patches starting from the first layer, while CNNs have limited receptive field and only learn global information
in top layers. Such strong global modeling ability can help learn the correspondence, when two objects are close in one
view while far from each other in the other view. 3) Since each patch has an explicit position embedding, it is possible to apply
non-uniform cropping, which removes arbitrary patches without changing the input of other patches, while CNNs can only apply
uniform cropping (i.e. cropping a rectangle area). Such flexibility of patch selection is beneficial for geo-localization.
Since some objects in aerial-view may not appear in street view due to occlusion, they can be removed with non-uniform cropping
to reduce computation and GPU memory footprint, while keeping the position information of other patches.

However, vanilla vision transformer [7] has some limitations on training data size and memory consumption, which must be addressed
when applied to cross-view geo-localization. The original ViT [7] requires extremely large training datasets to achieve state-of-the-
art, e.g. JFT-300M[7] or ImageNet-21K[5] (a super set of the original ImageNet-1K). It does not generalize well if trained on 
medium-scale datasets, because it does not have inductiev biases inherent in CNNs, e.g. shift-invariance and locality. Recently,
DeiT [27] applies strong data augmentation, knowledge distillation, and regularization techniques, in order to outperform CNN on 
ImageNet-1K, with similar parameters and inference throughout. However, mixup techniques used in DeiT are not straightforward for
metric learning losses [10].

In this paper, we propose the first pure transformer based method for cross-view geo-localization (TransGeo). To make our method 
more flexible without relying on data augmentations, we incorporate Adaptive Sharpness-Aware Minimization (ASAM), which avoids 
overfitting to local minima by optimizing the adaptive sharpness of loss landscape and ... Moreover, by analyzing the attention map
of top transformer encoder, we observe that most of the occluded regions in aerial images have negligible contribution to the output.
This motivates us to introduce the attention-guided non-uniform cropping, which first attends to informative image regions based
on attention map of transformer encoder, then increases the resolution only on the selected regions, resulting in an "attend
and zoom-in" procedure, similar to human vision. Our method achieves state-of-the-art performance with siginficant less computation
cost (GFLOPs) than CNN-based methods, e.g. SAFA [21]

We summarize our contributions as follows:

也就是说，过去的L2LTR, TransGeo, 虽然使用了Transformer, 但是TransGeo为了减少运算量提高算法的效率，它根据注意力权重删除了部分patch （这里我们可以拿
一些它的attention map去说，这些部分虽然权重低，但是还是包含有信息，如果直接删除会丢失信息），所以为了不丢失信息并且保持效率，我们选择了Swin-Transformer来
处理高像素图片的输入，因为Swin-Transformer的一大强项就是处理高像素图片

The first pure transformer-based method (TransGeo) for cross-view image geo-localization, without relying on polar transform or data
augmentation.

A novel attention-guided non-uniform cropping strategy that removes a large number of uninformative patches in reference aerial
images to reduce computation with negligible performance drop. The performance is further improved by reallocating the saved 
computation to higher image resolution of the informative regions.

State-of-the-art performance on both urban and rural datasets with less computation cost, GPU memory consumption, and inference
time than CNN-based methods.

Related Work:
Cross-view Geo-localization: Existing works for cross-view geo-localization generally adopt a two-stream CNN framework to extract
different features for two views, then learn an embedding space where images from the same GPS location are close to each other.
However, they fail to model the significant appearance gap between two views, resulting in poor retrieval performance. Recent
methods either leverage polar transform or add additional generative model [19, 26] to reduce domain gap by transforming
images from one view to the other. SAFA[21] designs a polar transform basedon the geometric prior knowlegde of the two views,
so that the transformed aerial images have similar layouts as street-view images. Toker et al. [26] further train a generative
network on the top of polar transform, so that the generated images are more realistic for matching. However, they highly 
rely on the geometric correspondence of two views.

On the other hand, several works start to consider practical scenarios where the street-view and aerial-view images are not
perfectly aligned in terms of orientation and spatial location. VIGOR [36] proposes a new urban dataset assuming that query can
occur at arbitrary locations in a given area, so the street-view image is not spatially aligned at the center of aerial image.
In such case, polar transform may fail to model the cross-view correspondence, due to unknown spatial shift and strong occlusion.
We show that vision transformer can tackle this challenging scenario with learnable position embedding on each input patch.

We notice that L2LTR [31] adopts vanilla ViT on the top of the ResNet [9], resulting in a hybrid CNN + Transformer
approach. Since it adopts CNN as feature extractor, the self-attention and position embedding are only used on
high-level CNN features, which does not fully exploit the global modeling ability and position information
from the first layer. Besides, as noted in their paper, it requires significantly larger GPU memory [31] and
pre-training dataset than CNN-based methods, while our approach enjoys GPU memory efficiency and uses the
same pre-training dataset as CNN-based methods, e.g. SAFA [21]. We compare with their method in Sec 4.3.

Vision Transformer: Transformer [28] is originally proposed for large-scale pre-training in NLP. It is firstly 
introduced for vision tasks in ViT [7] as vision transformer. ViT divides each input image into kxk small
patches, then considers each patch as one token along with position embedding and feeds them into multiple
transformer encoders. It requires extremely large training dataset to outperform CNN counterparts with similar
parameters, as it does not have inductive biases inherent in CNNs. DeiT [27] is recently proposed for data-
efficient training of vision transformer. It outperforms CNN counterparts on medium-scale datasets, i.e. 
ImageNet [5], by strong data augmentation and regularization techniques. A very recent work [4] further reduces
the augmentations to only inception-style augmentations [24]. However, even random crop could break the
spatial alignment, and previous works on cross-view geo-localization generally do not use any augmentation.
We aim to design a generic framework for cross-view geo-localization without any augmentation, thus introduce
a strong regularization technique, i.e. ASAM [11], to prevent vision transformer from overfitting.




-2-: VIGOR dataset:

: The previous ideal assumption when collecting ground and satellite images for geo-localization task is: for each collected
anchor panorama with specified location information, one satellite-view image whose center is at exactly same location will be sampled 
as the positive reference. However, it is not realistic to post-sample the 

Abstract:

Cross-view image geo-localization aims to determine the locations of street-view query images by matching with 
GPS-tagged reference images from aerial view. Recent works have achieved surprisingly high retrieval accuracy on city-
scale daatset. However, these results rely on the assumption that there exists a reference image exactly centered at
the location of any query image, which is not applicable for practical scenarios. In this paper, we redefine this problem
with a more realistic assumption that the query image can be arbitrary in the area of interest and the reference images
are captured before the queries emerge. This assumption breask the one-to-one retrieval setting of existing datasets
as the queries and reference images are not perfectly aligned pairs, and there may be multiple reference images covering
one query location. To bridge the gap bewteen this realistic setting and existing datasets, we propose a new large-scale
benchmark - VIGOR - for cross-view image geo-localization beyond one-to-one retrieval. We benchmark existing state-of-
the-art methods and propose a novel end-to-end framework to localize the query in a coarse-to-fine manner. Apart from
the image-level retrieval accuracy, we also evaluate the localization accuracy in terms of the actual distance (meters)
using the raw GPS data. Extensive experiments are conducted under different application scenarios to validate the
effectiveness of the proposed methods. The results indicate that cross-view geo-localization in this realistic setting
is still challenging, fostering new research in this direction.

Introduction:

The objective of image-based geo-localization is to determine the location of a query image by finding the most similar
image in a GPS-tagged reference database. Such technologies have proven useful for accurate localization with noisy GPS
signals [4,26] and navigation in crowded cities [12, 9]. Recently, there has been a surge of interest cross-view geo-
localization [], which uses GPS-tagged aerial-view images as reference for street-view queries. However, the performance
may suffer from a large view or appearance gap between query and reference images.

Recent works [7,17,29] have shown that the performance of cross-view image matching can be significantly improved by
feature aggregation and sample mining strategies. When the orientation of street-view (or ground-view) image is available (
provided by phone-based compass), state-of-the-art methods can achieve a top-1 retrieval accuracy over 80% [17], which
shows the possibility of accurate geo-localization in real-world settings. However, existing datasets [24, 27, 11] simply
assume that each query ground-view image has one corresponding reference aerial-view image whose center is exactly aligned
at the location of the query image. We argue this is not practical for real-world applications, because the query image
can occur at arbitrary locations in the area of interest and the reference images should be captured before the queries
emerge. In thsi case, perfectly aligned one-to-one correspondence is not guaranteedl

In light of the novelty of this problem, we propose a new benchmark (VIGOR) to evaluate cross-view geo-localization in a 
more realistic setting. Briefly, given an area of interest (AOI), the reference aerial images are densely sampled to
achieve a seamless coverage of the AOI and the street-view queries are captured at arbitrary locations. In total, 90,618
aerial images and 238,696 street panoramas are collected from 4 major cities in the United States. The new dataset gives
rise to two fundamental differences bewteen this work and prior research.

Beyond One to one: previous research mainly focuses on the one-to-one correspondence because existing datasets consider
perfectly aligned image pairs as default. However, VIGOR enables us to explore the effect of reference samples that are
not centered at the locations of queries but still cover the query area. As a result, there could be multiple reference
images partially covering the same query location, breaking the one-to-one correspondence. In our geo-localization method,
we design a novel hybrid loss to take advantages of multiple reference images during training.

Beyond Retrieval: Image retrieval can only provide image-level localization. Since the center alignment is not guaranteed 
in our dataset, after the retrieval, we further employ a within-image calibration to predict the offset of the query
location inside the retrieved image.
