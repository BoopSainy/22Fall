sampler that restricts data loading to a subset of the dataset.

it is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. in such a
case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a
subset of the original dataset that is exclusive to it.

现在有一个training dataset, 如果我有8个进程，那我希望我的8个进程能够各得一个subset of training dataset

shuffle -> padding -> distribute

dataset is assumed to be of constant size and that any instance of it always returns the same 
elements inthe same order.


arguments:
dataset: dataset used for sampling;
num_replicas: number of processes participating in distributed training. by default, world_size
is retrieved from the current distributed group.
rank: rank of the current process within num_replicas. by default, rank is retrieved from the
current distributed group.
shuffle: if True, sampler will shuffle the indices.

seed: random seed used to shuffle the sampler if shuffle=True, this number should be identical 
across all processes in the distributed group, default=0
(所有进程的sampler的随机种子需要一模一样，这应该是为了能让所进程在shuffle后还拥有一致的shuffled dataset

drop_last: if True, then the sampler will drop the tail of the data to make it evenly divisible
across the number of replicas. if False, the sampler will add extra indices to make the data
evenly divisible across the replicas. default: False,
(如果dataset有15个data,那我为了让8个进程分配到相同的数据，我要么就是drop掉7个，要么就是pad上1个重复的)

in distributed mode, calling the set_epoch() method at the begining of each epoch before creating
the DataLoader iterator is necessary to make shuffling work properly across multiple epochs.
otherwise, the same ordering will be always used.

在每个epoch开始的时候，在dataloader iterator生成之前，如果是distributed mode下的话，最好set_epoch()一下，
因为sampler的shuffle种子跟epoch有关系，如果不更新epoch的话，那么每个epoch sampler都会用同样的种子shuffle
dataset，那么每个epoch每个进程分配到的subset就会一样。


class DistributedSampler(Sampler[T_co]):
  """ sampler that restricts data loading to a subset of the dataset
  
  it is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel.
  in such a case, each process can pass a DistributedSampler instance as a DataLoader sampler,
  and load a subset of the original dataset that is exclusive to it
  
  Dataset is assumed to be of constant size and that any instance of it always returns 
  the same elements in the same order. 每个dataset都默认完全一致，甚至输出的elements也是在一个顺序之中
  
  def __init__(self, dataset: Dataset, num_replicas: Optional[int]=None, rank: Optional[int]=None,
              shuffle: bool=True, seed: int=0, drop_last: bool=False)->None:
    if num_replicas is None:#如果没有设定num_replicas是多少，也就是没设定要分多少份
      if not dist.is_available(): #分布式的package是否健全？
        raise RuntimeError("Requires distributed package to be available")
      num_replicas = dist.get_world_size()# 设定，分成world_size份，有多少进程分多少份
    if rank is None:
      if not dist.is_available():
        raise RuntimeError("Requires distributed package to be available")
      rank = dist.get_rank()#设定当前进程的rank
    if rank >= num_replicas or rank < 0:# rank不能大于等于总world_size
      raise ValueError(
          "Invalid rank {}, rank should be in the interval [0, {}]".format(rank, num_replicas-1)
    
    self.dataset = dataset #每个进程的distributed sampler都要拿到原始的完整dataset
    self.num_replicas = num_replicas #每个进程都需要知道一共要把dataset分成多少份
    self.rank = rank#每个进程需要知道自己的rank index
    self.epoch = 0#？？看看
    self.drop_last = drop_last
    
    # if the dataset length is evenly divisible by # of replicas, then there is no need
    # to drop any data, since the complete dataset will be split equally.
    if self.drop_last and len(self.dataset) % self.num_replicas !=0:
      # split to nearest available length that is evenly divisible.
      # this is to ensure each rank receives the same amount of data when using the Sampler;
      self.num_samples = math.ceil(
        (len(self.dataset) - self.num_replicas)/self.num_replicas) # 每个rank subset里有多少samples
    else:
      self.num_samples = math.ceil(len(self.dataset)/self.num_replicas) # pad原本的dataset
      # 使得它divisble
    self.total_size = self.num_samples * self.num_replicas # pad/drop后的dataset里有多少samples
    self.shuffle = shuffle #洗牌吗？
    self.seed = seed #怎么洗
    
  def __iter__(self) -> Iterator[T_co]:
    # 返回一个可循环的列表类变量
    if self.shuffle:
      # deterministically shuffle based on epoch and seed
      g = torch.Generator()
      g.manual_seed(self.seed + self.epoch) # 为每个进程，对于当前的epoch设定好shuffle种子
      indices = torch.randperm(len(self.dataset), generator=g).tolist() 
    else:
      indices = list(range(len(self.dataset)))
    
    if not self.drop_last:
      # add extra samples to make it evenly divisible if not drop_last
      padding_size = self.total_size - len(indices)
      if padding_size <= len(indices):
        indices += indices[:padding_size] #[0,7,3,1,2,10,8,4,5,12,14,9,6,11,13]+[0]
      else:#48-15=33; 33/15=3; 3*15=45; 45[:33]
        indices += (indices * math.ceil(padding_size/len(indices)))[:padding_size]
    else:
      # remove tail of data to make it evenly divisible.
      indices = indices[:self.total_size]
    assert len(indices) == self.total_size #更改过的indices长度应该等于total_size
    
    
    # subsample
    # 从rank开始，到最后结束，每隔num_replicas取一次
    indices = indices[self.rank:self.total_size:self.num_replicas]
    assert len(indices) == self.num_samples
    
    return iter(indices) # 返回一个iterator, 里面包含的是对应dataset里data的index
  
  def __len__(self) -> int:
    return self.num_samples
  
  def set_epoch(self, epoch:int) -> None:
    """
      sets the epoch for this sampler. when shuffle = True, this ensures all replicas
      use a different random ordering for each epoch. otherwise, the next ieration of this 
      sampler will yield the same ordering.
    """
    self.epoch = epoch
    
    
    
    
    
      
      
      
